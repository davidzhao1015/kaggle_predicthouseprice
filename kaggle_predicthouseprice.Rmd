---
title: 'Kaggle project: House price regression'
author: "Xin (David) Zhao"
date: "Last edited `r format(Sys.time(), '%d %B %Y')`"
knit: (function(inputFile, encoding) {
      out_dir <- 'docs';
      rmarkdown::render(inputFile,
                        encoding=encoding,
                        output_file=file.path(dirname(inputFile), out_dir, 'index.html'))})
output:
  html_document:
    # theme: cosmo
    highlight: pygments
    df_print: paged
    toc: TRUE
    toc_float: TRUE
    collapsed: FALSE
    number_sections: TRUE
    fig_width: 7
    fig_height: 6
    fig_caption: TRUE
editor_options: 
  markdown: 
    wrap: 72
# bibliography: references.bib
---

<html>

<head>

```{=html}
<style>

h1{
 color: #055C9D;
 font-family: Georgia;
 font-size: 200%
}


h2{
 color: #055C9D;
 font-family: helvetica;
 font-size: 150%
}

h3{
 color: #055C9D;  
 font-family: helvetica;
 font-size: 120%; 
}

p {
 color: #333333;
 font-family: helvetica;
 font-size: 100%;
}

.blackbox {
  padding: 1em;
  background: green;
  color: black;
  border: 2px solid orange;
  border-radius: 10px;
}

.center {
  text-align: center;
}

</style>
```
</head>

</html>

```{r setup, include = FALSE}
# set options for the entire document 
knitr::opts_chunk$set(fig.align = 'center', 
                      fig.height=6, fig.width=8,
                      dev="png",
                      echo=TRUE, #display code in output document 
                      error=FALSE,
                      collapse = FALSE, 
                      message=FALSE) #stop render when error occurs   
```

## Project aim

This is a Kaggle competition project. The purpose of this project is to
predict house prices in Ames, Iowa from available variables with machine
learning algorithms.

Personal learning purpose: - Reinforce machine learning knowledge -
Practice using caret package and workflow - Learn advanced feature
engineering - Learn new algorithms

## Workflow

Follow the machine learning workflow of caret R package.

0.  Load libraries
1.  Import data
2.  Exploratory data analysis
3.  Preprocess data - Create dummy variables - Zero- and near
    zero-variance - Impute missing data - Label encoding - Drop highly
    correlated variables - Linear dependencies - Centering and scaling -
    Transform predictors - Remove outliers
4.  Feature selection\
5.  Feature engineering\*
6.  Data splitting (skip)
7.  Train and tune models - Lasso regression model - XGBoost model -
    Random forest
8.  Evaluate performance

## R code

### Load R libraries

Load necessary libraries

```{r libraries, warning=FALSE}

library(tidyverse) 
library(caret) 
library(factoextra) # visualize k-means 
library(RANN)
library(mgcv) # generalized addictive model using splines - non-linear regression
library(ranger)  # random forest 

```

### Import data sets

```{r import data}

# unzip downloaded file 

unzip(zipfile = "./house-prices-advanced-regression-techniques.zip",
      files = NULL,
      exdir = "./raw-data")

list.files("./raw-data")

```

The row datasets contain four files: - `data_description.txt` -
`sample_submission.csv` - `test.csv` - `train.csv`

Import and inspect train set 

```{r csv files}

# read in train.csv

train_df <- read.csv(file = "./raw-data/train.csv",
                     header = T)

head(train_df)  # view first rows 


# data structure 
str(train_df)


dim(train_df)

```

The train set contains `r dim(train_df)[1]` observations and
`r dim(train_df)[2]` variables.

Find description of response and explanatory variables at the Kaggle [website](https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques/data). 


Import and inspect test set 

```{r test set}

# load test set 
test_df <- read.csv(file = "./raw-data/test.csv",
                    header = T)

head(test_df)

```

The test set contains `r dim(test_df)[1]` observations and
`r dim(test_df)[2]` variables.

### Exploratory data analysis 

#### Distribution of response variable in the train set 

The target variable to predict is `SalePrice` - the property's sale price in dollars.  

```{r  EDA-response-var}

# distribution of the response variable in the train set 
ggplot(data = train_df, aes(x = SalePrice/1000)) +
        geom_histogram(binwidth = 10, fill = "royalblue", alpha = 0.8) +
        scale_x_continuous(name = "Sale price in thousand dollars",
                           breaks = seq(1, 800, by=100),
                           labels = paste(seq(1, 800, by=100), "K", sep = "")) +
        geom_vline(aes(xintercept = 163), linetype = "dashed", color = "orange", size = 1)
        

```
The dashed line in the above histogram indicates the median of the sale prices in the train set. 


The response variable in the train set is left skewed. The average is 180K and median 163K. 

```{r summary-stats-response-var}

# summary stats 
summary(train_df$SalePrice)

# check if any missing values 
sum(is.na(train_df$SalePrice))  

```

### Numeric variables

```{r numeric-var}

# subset numeric variables 
train_df_num <- train_df %>% 
        dplyr::select(where(is.numeric))  


# pick top numeric variables based on rho 

res_num <- cor(train_df_num, method = "spearman")  

colnames(res_num) <- 1:ncol(res_num)

top_num_index <- res_num %>% 
        as.data.frame() %>% 
        rownames_to_column("variable") %>% 
        mutate(row_index = 1:nrow(.)) %>% 
        gather(key = "col_index", value = "rho", -c("row_index", "variable")) %>% 
        filter(variable == "SalePrice") %>% 
        filter(abs(rho) >= 0.5) %>%
        pull(col_index) %>% 
        as.numeric

top_num_var <- rownames(res_num)[top_num_index]  # top variable names  


```

Subset top number variables

```{r top-numeric-variables} 

caret::featurePlot(x = train_df_num[ ,top_num_index],
                   y = train_df_num$SalePrice,
                   plot = "Scatter",
                   layout = c(4,3),
                   jitter = T) 



```

### Factor variables 

Subset factor variables 

```{r subset-factor}

# covert character variables to factors 

train_df_fact <- train_df %>% 
        mutate_if(is.character, as.factor) %>% 
        dplyr::select(where(is.factor), SalePrice)    


str(train_df_fact) 


```

Subset 43 factor variables and the response variable `SalePrice`. 


Identify top factor variables associated with `SalePrice`. Use non-parameter [Kruskal-Wallis test](https://en.wikipedia.org/wiki/Kruskal%E2%80%93Wallis_one-way_analysis_of_variance#:~:text=Allen%20Wallis)%2C%20or%20one%2D,for%20comparing%20only%20two%20groups.) for multiple-class variables, while using [Mann-Whitney U test](https://en.wikipedia.org/wiki/Mann%E2%80%93Whitney_U_test) to binary-class variables. 

Use purrr package to apply function to columns of the data frame, referring to the [instruction](https://rpubs.com/faiazrmn/purrr_map_map2)

```{r top-factor-var}

# extract binary-class variables 
is_binary <- train_df_fact %>% 
        purrr::map_dfr(\(train_df_fact) length(levels(train_df_fact))) %>% 
        gather(key = "factor_variable", value = "levels") %>% 
        filter(levels == 2) %>% 
        pull(factor_variable) 


# apply Wilcox test to each variables 
train_df_fact_l2 <- train_df_fact %>% 
        select(all_of(is_binary), SalePrice) 

response_var <- train_df_fact_l2$SalePrice  %>% as.data.frame()

binary_var <- train_df_fact_l2[1:4] 

wilcox_fact_lv2 <- map2(binary_var, response_var, ~wilcox.test(.y ~ .x)) 


# extract p-value and subset significant variables 
wilcox_fact_lv2_df <- do.call(rbind, wilcox_fact_lv2) %>% 
        as.data.frame() %>% 
        rownames_to_column("variable") %>% 
        select(variable, p.value) %>% 
        filter(p.value <= 0.05) %>% 
        pull(variable)

```

For multiple-class factors, repeat the previous steps. 

```{r multiple-class-var-KW} 

# extract multi-class variables 
is_multiclass <- train_df_fact %>% 
        purrr::map_dfr(\(train_df_fact) length(levels(train_df_fact))) %>% 
        gather(key = "factor_variable", value = "levels") %>% 
        filter(levels > 2) %>% 
        pull(factor_variable) 


# apply kruskal-wallis test to each variables 
train_df_fact_multi <- train_df_fact %>% 
        select(all_of(is_multiclass), SalePrice) 

response_var <- train_df_fact_multi$SalePrice  %>% as.data.frame()

multiclass_var <- train_df_fact_multi %>% select(-SalePrice)

wilcox_fact_multi <- map2(multiclass_var, response_var, ~kruskal.test(.y ~ .x)) 


# extract p-value and subset significant variables 
wilcox_fact_multi_df <- do.call(rbind, wilcox_fact_multi) %>% 
        as.data.frame() %>% 
        rownames_to_column("variable") %>% 
        select(variable, p.value) %>% 
        filter(p.value <= 0.05) %>% 
        pull(variable)


```


### Create dummy variables 

Use `dummyVars` function in `caret` package to generate a complete set of dummy variables from multiple factors. 

Why creating dummy variables is necessary before subsequent ML modeling? 

```{r subset sig factors}

# subset significant factors from train set 

# train_df_fact_sig <- train_df_fact %>% 
#         select(all_of(wilcox_fact_lv2_df), 
#                all_of(wilcox_fact_multi_df),
#                SalePrice)
# 
# 
# dim(train_df_fact_sig) 


# convert character variables to factors in the train set 
train_df2 <- train_df %>% 
        mutate_if(is.character, as.factor) %>% 
        mutate(Id = paste("W", as.character(Id), sep = "")) %>%  # keep id as character variables 
        column_to_rownames("Id")  

```
The resulting dataset has 37 significant factors other than the response variable `SalePrice`. 

The function `model.matrix` creates a design (or model) matrix. [R documentation](https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/model.matrix) provides detailed information regarding the function. 


```{r dummy-var}

# head(model.matrix(SalePrice ~., data = train_df_fact_sig)) 

dummies_train <- dummyVars(SalePrice ~., data = train_df2)  

dummies_train_df <- data.frame(predict(dummies_train, newdata = train_df2)) 



# add SalePrice to the above data frame 

resp_df <- train_df2 %>% 
        rownames_to_column("Id") %>% 
        select(Id, SalePrice) 
        
dummies_train_df2 <- dummies_train_df %>% 
        rownames_to_column("Id") %>% 
        left_join(resp_df, by ="Id") 
        

```

As indicated by the caret package vignette, there is no intercept and each factor has a dummy variable for each level, so this parameterization may not be useful for some model functions, such as `lm`. 



### Near-zero variance 

According to the caret package documentation, a variable is considered the near-zero variance when it meets the conditions as follows, 

- "frequency ratio" is greater than a pre-specified threshold
- "unique value percentage" is less than a threshold 

The [website](https://topepo.github.io/caret/pre-processing.html#creating-dummy-variables) provide detailed explanation regarding near-zero variance.  


Identify zero- and near-zero variance variables, or either. 

```{r}

# near-zero variance 
nzv <- nearZeroVar(dummies_train_df2, saveMetrics = F) 

# drop nzv variables 
dummies_train_nzv <- dummies_train_df2[,-nzv] %>% 
        column_to_rownames("Id")

dim(dummies_train_df2)  # 290 variables 

dim(dummies_train_nzv)  # 126 variables after removing nvz variables 

```


Identify highly correlated variables.  

```{r highly-cor-var}

fl_train_cor <- cor(dummies_train_nzv) # pairwise correlation 

summary(fl_train_cor[upper.tri(fl_train_cor)]) # upper triangle of correlations matrix before the removal 


# replace NA with 0
fl_train_cor2 <- fl_train_cor %>% replace(is.na(.), 0)



# index of highly correlated variables 
highcorvar <- caret::findCorrelation(fl_train_cor2, cutoff = .75) 

which(colnames(dummies_train_nzv) == "SalePrice")

highcorvar2 <- highcorvar[highcorvar != 126]

# remove the highly correlated variables in the train set 
dummies_train_nzv2 <- dummies_train_nzv %>% 
        select(-highcorvar2)  



# # check correlation matrix after removing the highly correlated variables 
# dummies_train_nzv2_cor <- cor(dummies_train_nzv2)
# 
# summary(dummies_train_nzv2_cor[upper.tri(dummies_train_nzv2_cor)])

```




### Imuptation 

Caret package tutorial introduces two methods to impute missing values, K-nearest neighbors and bagged trees. 

The caret website provides detailed explanation. 

In this project, I used K-nearest neighbors to impute the missingness, if any. 

```{r imputation}

# distribution of missing values in data 
miss_value <- sapply(dummies_train_nzv2, function(x) sum(is.na(x))) %>% as.data.frame()

colnames(miss_value) <- "missing_count"

miss_value2 <- miss_value %>% 
  rownames_to_column("variable") %>% 
  filter(missing_count !=0) %>% 
  arrange(desc(missing_count)) 


top10_var_missing <- miss_value2 %>% 
  head(10) %>% 
  pull(variable)


# ggplot bar plot recording by missing_count 
ggplot(miss_value2, 
       aes(x = reorder(variable, missing_count), 
           y= missing_count)) +
  geom_bar(stat = "identity", fill = "royalblue") + 
  coord_flip()



dummies_train_missing <- dummies_train_nzv2 %>% 
  select(-all_of(top10_var_missing)) 



# apply k-nearest method to impute data 
impute_knn <- preProcess(dummies_train_missing,
                         method = "knnImpute") 

impute_knn


# train set after KNN imputation 
dummies_train_impute <- predict(impute_knn, 
                                newdata = dummies_train_missing) 


# I encountered an error when running the KNN impute method. I found a Stack Exchange post said that "the problem you run into is that knnImpute requires at least as many samples in your data without missing values as you have specified with the k parameter for the k-nearest-neighbors." 


```


## Feature selection 

### Clustering analysis - demontionality reduction 

Up next, perform unsupervised machine learning to selected numeric variables, 
- k-means clustering 
- hierarchical clustering 
- PCA 
In addition, compare two clustering assignments. 


Conduct k-means clustering analysis on the filtered variables in the train set. Refer to instructions on Datacamp and the website, [Datanovia](https://www.datanovia.com/en/lessons/k-means-clustering-in-r-algorith-and-practical-examples/). 

```{r kmeans-cluster}

# replace NA with 0
dummies_train_nzv3 <- dummies_train_impute %>% replace(is.na(.), 0)

# scale 
dummies_train_nzv3_scale <- scale(dummies_train_impute) 



# implement k-means 
set.seed(123)

kmeans_train <- kmeans(dummies_train_nzv3_scale, 
                       centers = 3,  # group the data into 3 clusters 
                       nstart = 20)  # generate 20 initial configuration 

summary(kmeans_train)


# distribution of k-means clustering 
table(kmeans_train$cluster)


# calculate mean of the response variable, SalePrice by the clusters 
aggregate(dummies_train_nzv, 
          by = list(cluster = kmeans_train$cluster), 
          mean) %>% 
        select(cluster, SalePrice)


```

The Cluster 2 has the greatest average compared to Cluster 1 and 3. 

Visualize kmeans clustering with `fviz_cluster` function in `functoextra` package. Refer to the online [instruction](https://uc-r.github.io/kmeans_clustering)


```{r viz kmeans}

fviz_cluster(kmeans_train, 
             data = dummies_train_nzv3_scale, 
             geom= "point") 

```



Determine optimal cluster with the Elbow plot. Refer to the online [codes](https://uc-r.github.io/kmeans_clustering#elbow)

```{r best-cluster}

set.seed(123)

# function to compute total within-cluster sum of square 
wss <- function(k) {
  kmeans(dummies_train_nzv3_scale, k, nstart = 10 )$tot.withinss
}

# Compute and plot wss for k = 1 to k = 15
k.values <- 1:15

# extract wss for 2-15 clusters
wss_values <- map_dbl(k.values, wss)

plot(k.values, wss_values,
       type="b", pch = 19, frame = FALSE, 
       xlab="Number of clusters K",
       ylab="Total within-clusters sum of squares")

```



Run PCA on the filtered variables in the train-set. 

```{r PCA}

set.seed(123)

pr.train <- prcomp(x = dummies_train_nzv3, scale =T, center = T)   

summary(pr.train)


```


```{r biplot pca}

# # base R plot 
# biplot(pr.train,
#        xlim = c(-.1, .1), 
#        expand = 2,
#        main = "PCA of house sale price data",
#        xlab = "PC1",
#        ylab = "PC2")


# ggplot2-based plot for PCA analysis 
fviz_pca_biplot(pr.train, 
                repel = F,
                col.ind = "grey",
                label = "var",
                select.var = list(contrib = 10)) # show top 10 variables   

```

Extract loading from PCA analysis using `factoextra` package. 

```{r PCA loadings}

# extract contribution of variables 
pca_var <- factoextra::get_pca_var(pr.train) 


# top 10 variables that explain variations of data 
top_var_pca <- pca_var$contrib  %>% 
  as.data.frame() %>% 
  arrange(desc(abs(Dim.1))) %>% 
  rownames_to_column("Variable") %>% 
  head(12) %>% 
  pull(Variable)

top_var_pca 

```

9 variables most correlated with PC1 other than `SalePrice` are as follows, from most to least

- `YearBuild` Original construction date 
- `OverallQual` Overall material and finish quality 
- `YearRemodAdd` Remodel date 
- `BsmtQual.TA` Height of the basement (TA)
- `ExterQual.Gd` Exterior material quality (Gd) 
- `GarageArea` Size of garage in square feet 
- `FullBath`Full bathrooms above grade 
- `GarageFinish.Unf`Interior finish of the garage (Unf)
- `HeatingQC.Ex`Heating quality and condition (Ex)
- `BsmtFinType1.GLQ`Quality of basement finished area (GLQ)
- `GarageType.Detchd` Garage location (Detchd)

`BsmtQual.TA` links to low house sale price, whereas remaining variables link to high sale price. 

<!-- Insert an external resources to explain how to interpret PCA biplot.   -->



### Univariate analysis to select numeric variables 

```{r univariate}

# subset numeric variables 
dummies_train_impute_num <- dummies_train_impute %>% 
        dplyr::select(where(is.numeric))  


# pick top numeric variables based on rho 

res_num <- cor(dummies_train_impute_num, method = "spearman")  

colnames(res_num) <- 1:ncol(res_num)

top_num_index <- res_num %>% 
        as.data.frame() %>% 
        rownames_to_column("variable") %>% 
        mutate(row_index = 1:nrow(.)) %>% 
        gather(key = "col_index", value = "rho", -c("row_index", "variable")) %>% 
        filter(variable == "SalePrice") %>% 
        filter(abs(rho) >= 0.5) %>%
        arrange(desc(abs(rho))) %>% 
        pull(col_index) %>% 
        as.numeric

top_num_var <- rownames(res_num)[top_num_index]  # top variable names  


```

Univariate analysis selected 13 numeric variables significantly associated with `SalePrice`, 
- `OverallQual` 
- `YearBuilt`
- `GarageArea`
-`FullBath`
- `GarageYrBlt`
-`GarageFinish.Unf`
-`X1stFlrSF`
-`ExterQual.Gd`
-`BsmtQual.TA`
-`YearRemodAdd`
-`TotRmsAbvGrd`
-`GarageType.Detchd`
-`Fireplaces` 


### Train models 

Set up resampling method. Use [k-fold cross validation](https://rpubs.com/cliex159/881990) strategy. 


```{r cross validation}

fitControl <- trainControl(method = "cv", # cross validation 
                           number = 10,  # k = 10 
                           verboseIter = TRUE)  

```



```{r select predictors}

# # select some numeric variables 
# select_variables <- c("SalePrice", "YearBuilt", "GarageArea", "GarageYrBlt", "X1stFlrSF", "YearRemodAdd") 


# prepare the final train set 
dummies_train_missing2 <- 
  dummies_train_missing %>% 
  select(all_of(top_num_var))  

dummies_train_missing3 <- dummies_train_missing2 %>% replace(is.na(.), 0)


head(dummies_train_missing3) 

```

Train models 

```{r train models}

set.seed(123)

# eXtreme Gradient Boosting - installing the package takes time 
# xgb <- train(SalePrice ~ ., 
#              data = dummies_train_impute2,
#              method = "xgbDART",
#              trControl = fitControl,
#              verbose = FALSE)


# Generalized linear model
glm_train <- train(SalePrice ~ .,
                   data = dummies_train_missing3,
                   method = "glm",
                   trControl = fitControl) 

glm_train # train model   

# dummies_train_missing3$pred_glm <- predict(glm_house) 
# 
# ggplot(dummies_train_missing3, aes(x = SalePrice, y= pred_glm)) +
#   geom_point()+
#   geom_abline()+
#   geom_smooth(se = F) +
#   ggtitle("Sale price vs. glm model prediction")
# 
# # RMSE vs. standard deviations of Sale price 
# # RMSE = 38900.37 
# # sd = 79442.5 
# 
# sd(dummies_train_missing3$SalePrice)


# Linear regression 
lm_train <- train(SalePrice ~ .,
                   data = dummies_train_missing3,
                   method = "lm",
                   trControl = fitControl) 

lm_train 


```

Build different types of models: 
- linear regression 
- GAM model (Generalized additive model)
- Random Forest 
- Gradient boosting machines 

```{r build more models}

set.seed(123)

# GAM 
gam_train <- train(SalePrice ~ .,
                   data = dummies_train_missing3,
                   method = "gam",
                   trControl = fitControl)

gam_train # model information 



# Random forest 
rf_train <- train(SalePrice ~ .,
                   data = dummies_train_missing3,
                   method = "rf",
                   trControl = fitControl) 

rf_train # model informatin 


```


<!-- Plots:  -->
<!-- - Prediction vs. ground truth plot  -->
<!-- - Residual plot  -->
<!-- - Gain curve  -->


<!-- Select the best model  -->


<!-- Preprocess unseen data  -->

<!-- Predict unseen data and performance metrics  -->


<!-- Tuning model parameters to improve performance  -->































