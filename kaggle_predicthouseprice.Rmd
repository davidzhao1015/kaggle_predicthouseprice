---
title: 'Kaggle project: House price regression'
author: "Xin (David) Zhao"
date: "Last edited `r format(Sys.time(), '%d %B %Y')`"
knit: (function(inputFile, encoding) {
      out_dir <- 'docs';
      rmarkdown::render(inputFile,
                        encoding=encoding,
                        output_file=file.path(dirname(inputFile), out_dir, 'index.html'))})
output:
  html_document:
    # theme: cosmo
    highlight: pygments
    df_print: paged
    toc: TRUE
    toc_float: TRUE
    collapsed: FALSE
    number_sections: TRUE
    fig_width: 7
    fig_height: 6
    fig_caption: TRUE
editor_options: 
  markdown: 
    wrap: 72
# bibliography: references.bib
---

<html>

<head>

```{=html}
<style>

h1{
 color: #055C9D;
 font-family: Georgia;
 font-size: 200%
}


h2{
 color: #055C9D;
 font-family: helvetica;
 font-size: 150%
}

h3{
 color: #055C9D;  
 font-family: helvetica;
 font-size: 120%; 
}

p {
 color: #333333;
 font-family: helvetica;
 font-size: 100%;
}

.blackbox {
  padding: 1em;
  background: green;
  color: black;
  border: 2px solid orange;
  border-radius: 10px;
}

.center {
  text-align: center;
}

</style>
```
</head>

</html>

```{r setup, include = FALSE}
# set options for the entire document 
knitr::opts_chunk$set(fig.align = 'center', 
                      fig.height=6, fig.width=8,
                      dev="png",
                      echo=TRUE, #display code in output document 
                      error=FALSE,
                      collapse = FALSE, 
                      message=FALSE) #stop render when error occurs   
```

## Project aim

This is a Kaggle competition project. The purpose of this project is to
predict house prices in Ames, Iowa from available variables with machine
learning algorithms.

Personal learning purpose: - Reinforce machine learning knowledge -
Practice using caret package and workflow - Learn advanced feature
engineering - Learn new algorithms

## Workflow

Follow the machine learning workflow of caret R package.

0.  Load libraries
1.  Import data
2.  Exploratory data analysis
3.  Preprocess data - Create dummy variables - Zero- and near
    zero-variance - Impute missing data - Label encoding - Drop highly
    correlated variables - Linear dependencies - Centering and scaling -
    Transform predictors - Remove outliers
4.  Feature selection\
5.  Feature engineering\*
6.  Data splitting (skip)
7.  Train and tune models - Lasso regression model - XGBoost model -
    Random forest
8.  Evaluate performance

## R code

### Load R libraries

Load necessary libraries

```{r libraries, warning=FALSE}

library(tidyverse) 
library(caret) 
library(factoextra) # visualize kmeans 

```

### Import data sets

```{r import data}

# unzip downloaded file 

unzip(zipfile = "./house-prices-advanced-regression-techniques.zip",
      files = NULL,
      exdir = "./raw-data")

list.files("./raw-data")

```

The row datasets contain four files: - `data_description.txt` -
`sample_submission.csv` - `test.csv` - `train.csv`

Import and inspect train set 

```{r csv files}

# read in train.csv

train_df <- read.csv(file = "./raw-data/train.csv",
                     header = T)

head(train_df)  # view first rows 


# data structure 
str(train_df)


dim(train_df)

```

The train set contains `r dim(train_df)[1]` observations and
`r dim(train_df)[2]` variables.

Find description of response and explanatory variables at the Kaggle [website](https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques/data). 


Import and inspect test set 

```{r test set}

# load test set 
test_df <- read.csv(file = "./raw-data/test.csv",
                    header = T)

head(test_df)

```

The test set contains `r dim(test_df)[1]` observations and
`r dim(test_df)[2]` variables.

### Exploratory data analysis 

#### Distribution of response variable in the train set 

The target variable to predict is `SalePrice` - the property's sale price in dollars.  

```{r  EDA-response-var}

# distribution of the response variable in the train set 
ggplot(data = train_df, aes(x = SalePrice/1000)) +
        geom_histogram(binwidth = 10, fill = "royalblue", alpha = 0.8) +
        scale_x_continuous(name = "Sale price in thousand dollars",
                           breaks = seq(1, 800, by=100),
                           labels = paste(seq(1, 800, by=100), "K", sep = "")) +
        geom_vline(aes(xintercept = 163), linetype = "dashed", color = "orange", size = 1)
        

```
The dashed line in the above histogram indicates the median of the sale prices in the train set. 


The response variable in the train set is left skewed. The average is 180K and median 163K. 

```{r summary-stats-response-var}

# summary stats 
summary(train_df$SalePrice)

# check if any missing values 
sum(is.na(train_df$SalePrice))  

```

### Numeric variables

```{r numeric-var}

# subset numeric variables 
train_df_num <- train_df %>% 
        dplyr::select(where(is.numeric))  


# pick top numeric variables based on rho 

res_num <- cor(train_df_num, method = "spearman")  

colnames(res_num) <- 1:ncol(res_num)

top_num_index <- res_num %>% 
        as.data.frame() %>% 
        rownames_to_column("variable") %>% 
        mutate(row_index = 1:nrow(.)) %>% 
        gather(key = "col_index", value = "rho", -c("row_index", "variable")) %>% 
        filter(variable == "SalePrice") %>% 
        filter(abs(rho) >= 0.5) %>%
        pull(col_index) %>% 
        as.numeric

top_num_var <- rownames(res_num)[top_num_index]  # top variable names  


```

Subset top number variables

```{r top-numeric-variables} 

caret::featurePlot(x = train_df_num[ ,top_num_index],
                   y = train_df_num$SalePrice,
                   plot = "Scatter",
                   layout = c(4,3),
                   jitter = T) 



```

### Factor variables 

Subset factor variables 

```{r subset-factor}

# covert character variables to factors 

train_df_fact <- train_df %>% 
        mutate_if(is.character, as.factor) %>% 
        dplyr::select(where(is.factor), SalePrice)    


str(train_df_fact) 


```

Subset 43 factor variables and the response variable `SalePrice`. 


Identify top factor variables associated with `SalePrice`. Use non-parameter [Kruskal-Wallis test](https://en.wikipedia.org/wiki/Kruskal%E2%80%93Wallis_one-way_analysis_of_variance#:~:text=Allen%20Wallis)%2C%20or%20one%2D,for%20comparing%20only%20two%20groups.) for multiple-class variables, while using [Mann-Whitney U test](https://en.wikipedia.org/wiki/Mann%E2%80%93Whitney_U_test) to binary-class variables. 

Use purrr package to apply function to columns of the data frame, referring to the [instruction](https://rpubs.com/faiazrmn/purrr_map_map2)

```{r top-factor-var}

# extract binary-class variables 
is_binary <- train_df_fact %>% 
        purrr::map_dfr(\(train_df_fact) length(levels(train_df_fact))) %>% 
        gather(key = "factor_variable", value = "levels") %>% 
        filter(levels == 2) %>% 
        pull(factor_variable) 


# apply Wilcox test to each variables 
train_df_fact_l2 <- train_df_fact %>% 
        select(all_of(is_binary), SalePrice) 

response_var <- train_df_fact_l2$SalePrice  %>% as.data.frame()

binary_var <- train_df_fact_l2[1:4] 

wilcox_fact_lv2 <- map2(binary_var, response_var, ~wilcox.test(.y ~ .x)) 


# extract p-value and subset significant variables 
wilcox_fact_lv2_df <- do.call(rbind, wilcox_fact_lv2) %>% 
        as.data.frame() %>% 
        rownames_to_column("variable") %>% 
        select(variable, p.value) %>% 
        filter(p.value <= 0.05) %>% 
        pull(variable)

```

For multiple-class factors, repeat the previous steps. 

```{r multiple-class-var-KW} 

# extract multi-class variables 
is_multiclass <- train_df_fact %>% 
        purrr::map_dfr(\(train_df_fact) length(levels(train_df_fact))) %>% 
        gather(key = "factor_variable", value = "levels") %>% 
        filter(levels > 2) %>% 
        pull(factor_variable) 


# apply kruskal-wallis test to each variables 
train_df_fact_multi <- train_df_fact %>% 
        select(all_of(is_multiclass), SalePrice) 

response_var <- train_df_fact_multi$SalePrice  %>% as.data.frame()

multiclass_var <- train_df_fact_multi %>% select(-SalePrice)

wilcox_fact_multi <- map2(multiclass_var, response_var, ~kruskal.test(.y ~ .x)) 


# extract p-value and subset significant variables 
wilcox_fact_multi_df <- do.call(rbind, wilcox_fact_multi) %>% 
        as.data.frame() %>% 
        rownames_to_column("variable") %>% 
        select(variable, p.value) %>% 
        filter(p.value <= 0.05) %>% 
        pull(variable)


```


### Create dummy variables 

Use `dummyVars` function in `caret` package to generate a complete set of dummy variables from multiple factors. 

Why creating dummy variables is necessary before subsequent ML modeling? 

```{r subset sig factors}

# subset significant factors from train set 

# train_df_fact_sig <- train_df_fact %>% 
#         select(all_of(wilcox_fact_lv2_df), 
#                all_of(wilcox_fact_multi_df),
#                SalePrice)
# 
# 
# dim(train_df_fact_sig) 


# convert character variables to factors in the train set 
train_df2 <- train_df %>% 
        mutate_if(is.character, as.factor) %>% 
        mutate(Id = paste("W", as.character(Id), sep = "")) %>%  # keep id as character variables 
        column_to_rownames("Id")  

```
The resulting dataset has 37 significant factors other than the response variable `SalePrice`. 

The function `model.matrix` creates a design (or model) matrix. [R documentation](https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/model.matrix) provides detailed information regarding the function. 


```{r dummy-var}

# head(model.matrix(SalePrice ~., data = train_df_fact_sig)) 

dummies_train <- dummyVars(SalePrice ~., data = train_df2)  

dummies_train_df <- data.frame(predict(dummies_train, newdata = train_df2)) 



# add SalePrice to the above data frame 

resp_df <- train_df2 %>% 
        rownames_to_column("Id") %>% 
        select(Id, SalePrice) 
        
dummies_train_df2 <- dummies_train_df %>% 
        rownames_to_column("Id") %>% 
        left_join(resp_df, by ="Id") 
        

```

As indicated by the caret package vignette, there is no intercept and each factor has a dummy variable for each level, so this parameterization may not be useful for some model functions, such as `lm`. 



### Near-zero variance 

According to the caret package documentation, a variable is considered the near-zero variance when it meets the conditions as follows, 

- "frequency ratio" is greater than a pre-specified threshold
- "unique value percentage" is less than a threshold 

The [website](https://topepo.github.io/caret/pre-processing.html#creating-dummy-variables) provide detailed explanation regarding near-zero variance.  


Identify zero- and near-zero variance variables, or either. 

```{r }

# near-zero variance 
nzv <- nearZeroVar(dummies_train_df2, saveMetrics = F) 

# drop nzv variables 
dummies_train_nzv <- dummies_train_df2[,-nzv] %>% 
        column_to_rownames("Id")

dim(dummies_train_df2)  # 290 variables 

dim(dummies_train_nzv)  # 126 variables after removing nvz variables 

```


Identify highly correlated variables.  

```{r highly-cor-var}

fl_train_cor <- cor(dummies_train_nzv) # pairwise correlation 

summary(fl_train_cor[upper.tri(fl_train_cor)]) # upper triangle of correlations matrix before the removal 


# replace NA with 0
fl_train_cor2 <- fl_train_cor %>% replace(is.na(.), 0)



# index of highly correlated variables 
highcorvar <- caret::findCorrelation(fl_train_cor2, cutoff = .75) 


# remove the highly correlated variables in the train set 
dummies_train_nzv2 <- dummies_train_nzv %>% 
        select(-highcorvar)  



# # check correlation matrix after removing the highly correlated variables 
# dummies_train_nzv2_cor <- cor(dummies_train_nzv2)
# 
# summary(dummies_train_nzv2_cor[upper.tri(dummies_train_nzv2_cor)])

```


### Clustering analysis - demontionality reduction 
Up next, perform unsupervised machine learning to selected numeric variables, 
- k-means clustering 
- hierarchical clustering 
- PCA 
In addition, compare two clustering assignments. 


Conduct k-means clustering analysis on the filtered variables in the train set. Refer to instructions on Datacamp and the website, [Datanovia](https://www.datanovia.com/en/lessons/k-means-clustering-in-r-algorith-and-practical-examples/). 

```{r kmeans-cluster}

# replace NA with 0 
dummies_train_nzv3 <- dummies_train_nzv2 %>% replace(is.na(.), 0) 

# scale 
dummies_train_nzv3_scale <- scale(dummies_train_nzv3) 



# implement kmeans 
set.seed(123)

kmeans_train <- kmeans(dummies_train_nzv3_scale, 
                       centers = 3,  # group the data into 3 clusters 
                       nstart = 20)  # generate 20 initial configuration 

summary(kmeans_train)



# distribution of kmeans clustering 
table(kmeans_train$cluster)


# calculate mean of the response variable, SalePrice by the clusters 
aggregate(dummies_train_nzv, by = list(cluster = kmeans_train$cluster), mean) %>% 
        select(cluster, SalePrice)


```

The Cluster 2 has the greatest average compared to Cluster 1 and 3. 

Visualize kmeans clustering with `fviz_cluster` function in `functoextra` package. Refer to the online [instruction](https://uc-r.github.io/kmeans_clustering)


```{r viz kmeans}

fviz_cluster(kmeans_train, data = dummies_train_nzv3_scale, geom= "point")

```



Determine optimal cluster with the Elbow plot. Refer to the online [codes](https://uc-r.github.io/kmeans_clustering#elbow)

```{r best-cluster}

set.seed(123)

# function to compute total within-cluster sum of square 
wss <- function(k) {
  kmeans(dummies_train_nzv3_scale, k, nstart = 10 )$tot.withinss
}

# Compute and plot wss for k = 1 to k = 15
k.values <- 1:15

# extract wss for 2-15 clusters
wss_values <- map_dbl(k.values, wss)

plot(k.values, wss_values,
       type="b", pch = 19, frame = FALSE, 
       xlab="Number of clusters K",
       ylab="Total within-clusters sum of squares")




```





Run PCA on the filtered variables in the train set. 

```{r PCA}

dummies_train_nzv_rm <- dummies_train_nzv %>% replace(is.na(.), 0)

pr.train <- prcomp(x = dummies_train_nzv_rm, scale =T, center = T)   

summary(pr.train)

```


```{r biplot pca}

# base R plot 
biplot(pr.train,
       xlim = c(-.1, .1), 
       expand = 2,
       main = "PCA of house sale price data",
       xlab = "PC1",
       ylab = "PC2")



# ggplot2-based plot 

fviz_pca_biplot(pr.train, 
                repel = F,
                col.ind = "grey",
                label = "var",
                select.var = list(contrib = 10)) # show top 10 variables   

 


```








