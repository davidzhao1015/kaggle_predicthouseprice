---
title: 'Kaggle project: House price regression'
author: "Xin (David) Zhao"
date: "Last edited `r format(Sys.time(), '%d %B %Y')`"
knit: (function(inputFile, encoding) {
      out_dir <- 'docs';
      rmarkdown::render(inputFile,
                        encoding=encoding,
                        output_file=file.path(dirname(inputFile), out_dir, 'index.html'))})
output:
  html_document:
    # theme: cosmo
    highlight: pygments
    df_print: paged
    toc: TRUE
    toc_float: TRUE
    collapsed: FALSE
    number_sections: TRUE
    fig_width: 7
    fig_height: 6
    fig_caption: TRUE
editor_options: 
  markdown: 
    wrap: 72
# bibliography: references.bib
---

<html>

<head>

```{=html}
<style>

h1{
 color: #055C9D;
 font-family: Georgia;
 font-size: 200%
}


h2{
 color: #055C9D;
 font-family: helvetica;
 font-size: 150%
}

h3{
 color: #055C9D;  
 font-family: helvetica;
 font-size: 120%; 
}

p {
 color: #333333;
 font-family: helvetica;
 font-size: 100%;
}

.blackbox {
  padding: 1em;
  background: green;
  color: black;
  border: 2px solid orange;
  border-radius: 10px;
}

.center {
  text-align: center;
}

</style>
```
</head>

</html>

```{r setup, include = FALSE}
# set options for the entire document 
knitr::opts_chunk$set(fig.align = 'center', 
                      fig.height=6, fig.width=8,
                      dev="png",
                      echo=TRUE, #display code in output document 
                      error=FALSE,
                      collapse = FALSE, 
                      message=FALSE) #stop render when error occurs   
```

## Project aim

This is a Kaggle competition project. The purpose of this project is to
predict house prices in Ames, Iowa from available variables with machine
learning algorithms.

Personal learning purpose: - Refresh machine learning knowledge -
Practice using caret package and workflow - Learn advanced feature
engineering - Learn math and principles for algorithms

## Workflow

Follow the machine learning workflow of caret R package.

0.  Load libraries
1.  Import data
2.  Exploratory data analysis
3.  Preprocess data - Create dummy variables - Zero- and near
    zero-variance - Impute missing data - Label encoding - Drop highly
    correlated variables - Linear dependencies - Centering and scaling -
    Transform predictors - Remove outliers
4.  Feature selection\
5.  Feature engineering\*
6.  Data splitting (skip)
7.  Train and tune models - Lasso regression model - XGBoost model -
    Random forest
8.  Evaluate performance

## R code

### Load R libraries

Load necessary libraries

```{r libraries, warning=FALSE}

library(tidyverse) 
library(caret) 

```

### Import data sets

```{r import data}

# unzip downloaded file 

unzip(zipfile = "./house-prices-advanced-regression-techniques.zip",
      files = NULL,
      exdir = "./raw-data")

list.files("./raw-data")

```

The row datasets contain four files: - `data_description.txt` -
`sample_submission.csv` - `test.csv` - `train.csv`

Import and inspect train set 

```{r csv files}

# read in train.csv

train_df <- read.csv(file = "./raw-data/train.csv",
                     header = T)

head(train_df)  # view first rows 


# data structure 
str(train_df)


dim(train_df)

```

The train set contains `r dim(train_df)[1]` observations and
`r dim(train_df)[2]` variables.

Find description of response and explanatory variables at the Kaggle [website](https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques/data). 


Import and inspect test set 

```{r test set}

# load test set 
test_df <- read.csv(file = "./raw-data/test.csv",
                    header = T)

head(test_df)

```

The test set contains `r dim(test_df)[1]` observations and
`r dim(test_df)[2]` variables.

### Exploratory data analysis 

#### Distribution of response variable in the train set 

The target variable to predict is `SalePrice` - the property's sale price in dollars.  

```{r  EDA-response-var}

# distribution of the response variable in the train set 
ggplot(data = train_df, aes(x = SalePrice/1000)) +
        geom_histogram(binwidth = 10, fill = "royalblue", alpha = 0.8) +
        scale_x_continuous(name = "Sale price in thousand dollars",
                           breaks = seq(1, 800, by=100),
                           labels = paste(seq(1, 800, by=100), "K", sep = "")) +
        geom_vline(aes(xintercept = 163), linetype = "dashed", color = "orange", size = 1)
        

```
The dashed line in the above histogram indicates the median of the sale prices in the train set. 


The response variable in the train set is left skewed. The average is 180K and median 163K. 

```{r summary-stats-response-var}

# summary stats 
summary(train_df$SalePrice)

# check if any missing values 
sum(is.na(train_df$SalePrice))  

```

### Numeric variables

```{r numeric-var}

# subset numeric variables 
train_df_num <- train_df %>% 
        dplyr::select(where(is.numeric))  


# pick top numeric variables based on rho 

res_num <- cor(train_df_num, method = "spearman")  

colnames(res_num) <- 1:ncol(res_num)

top_num_index <- res_num %>% 
        as.data.frame() %>% 
        rownames_to_column("variable") %>% 
        mutate(row_index = 1:nrow(.)) %>% 
        gather(key = "col_index", value = "rho", -c("row_index", "variable")) %>% 
        filter(variable == "SalePrice") %>% 
        filter(abs(rho) >= 0.5) %>%
        pull(col_index) %>% 
        as.numeric

top_num_var <- rownames(res_num)[top_num_index]  # top variable names  


```

Subset top number variables

```{r top-numeric-variables} 

caret::featurePlot(x = train_df_num[ ,top_num_index],
                   y = train_df_num$SalePrice,
                   plot = "Scatter",
                   layout = c(4,3),
                   jitter = T) 



```

### Factor variables 

Subset factor variables 

```{r subset-factor}

# covert character variables to factors 

train_df_fact <- train_df %>% 
        mutate_if(is.character, as.factor) %>% 
        dplyr::select(where(is.factor), SalePrice)    


str(train_df_fact) 


```

Subset 43 factor variables and the response variable `SalePrice`. 


Identify top factor variables associated with `SalePrice`. Use non-parameter [Kruskal-Wallis test](https://en.wikipedia.org/wiki/Kruskal%E2%80%93Wallis_one-way_analysis_of_variance#:~:text=Allen%20Wallis)%2C%20or%20one%2D,for%20comparing%20only%20two%20groups.) for multiple-class variables, while using [Mann-Whitney U test](https://en.wikipedia.org/wiki/Mann%E2%80%93Whitney_U_test) to binary-class variables. 

Use purrr package to apply function to columns of the data frame, referring to the [instruction](https://rpubs.com/faiazrmn/purrr_map_map2)

```{r top-factor-var}

# extract binary-class variables 
is_binary <- train_df_fact %>% 
        purrr::map_dfr(\(train_df_fact) length(levels(train_df_fact))) %>% 
        gather(key = "factor_variable", value = "levels") %>% 
        filter(levels == 2) %>% 
        pull(factor_variable) 


# apply Wilcox test to each variables 
train_df_fact_l2 <- train_df_fact %>% 
        select(all_of(is_binary), SalePrice) 

response_var <- train_df_fact_l2$SalePrice  %>% as.data.frame()

binary_var <- train_df_fact_l2[1:4] 

wilcox_fact_lv2 <- map2(binary_var, response_var, ~wilcox.test(.y ~ .x)) 


# extract p-value and subset significant variables 
wilcox_fact_lv2_df <- do.call(rbind, wilcox_fact_lv2) %>% 
        as.data.frame() %>% 
        rownames_to_column("variable") %>% 
        select(variable, p.value) %>% 
        filter(p.value <= 0.05) %>% 
        pull(variable)


```

For multiple-class factors, repeat the previous steps. 

```{r multiple-class-var-KW} 

# extract binary-class variables 
is_multiclass <- train_df_fact %>% 
        purrr::map_dfr(\(train_df_fact) length(levels(train_df_fact))) %>% 
        gather(key = "factor_variable", value = "levels") %>% 
        filter(levels > 2) %>% 
        pull(factor_variable) 


# apply kruskal-wallis test to each variables 
train_df_fact_multi <- train_df_fact %>% 
        select(all_of(is_multiclass), SalePrice) 

response_var <- train_df_fact_multi$SalePrice  %>% as.data.frame()

multiclass_var <- train_df_fact_multi %>% select(-SalePrice)

wilcox_fact_multi <- map2(multiclass_var, response_var, ~kruskal.test(.y ~ .x)) 


# extract p-value and subset significant variables 
wilcox_fact_multi_df <- do.call(rbind, wilcox_fact_multi) %>% 
        as.data.frame() %>% 
        rownames_to_column("variable") %>% 
        select(variable, p.value) %>% 
        filter(p.value <= 0.05) %>% 
        pull(variable)


```

Identify zero- and near-zero variance variables, or either. 

```{r }

# near-zero variance 
nzv <- nearZeroVar(train_df_num[ ,top_num_index], saveMetrics = F) 

dim(train_df_num[ ,top_num_index])



```

Identify highly correlated variables.  

```{r highly-cor-var}

fl_train_cor <- cor(train_df_num[ ,top_num_index])

summary(fl_train_cor[upper.tri(fl_train_cor)])


highcorvar <- findCorrelation(fl_train_cor, cutoff = .75) 


train_df_num_top <- train_df_num[ ,top_num_index] %>% 
        select(-highcorvar)  


train_df_num_top_cor <- cor(train_df_num_top)

summary(train_df_num_top_cor[upper.tri(train_df_num_top_cor)])

```

Up next, perform unsupervised machine learning to selected numeric variables, 
- k-means clustering 
- hierarchical clustering 
- PCA 

In addition, compare two clustering assignments. 




